---
title: "Регрессионный анализ, часть 2"
subtitle: "Математические методы в зоологии с использованием R"
author: "Марина Варфоломеева"
output:
  beamer_presentation:
    colortheme: seagull
    highlight: tango
    fonttheme: structurebold
    includes:
      in_header: ./includes/header.tex
    pandoc_args:
    - --latex-engine=xelatex
    - -V fontsize=10pt
    - -V lang=russian
    slide_level: 2
    theme: CambridgeUS
    toc: yes
---

```{r setup, include = FALSE, cache = FALSE, purl = FALSE}
options(width = 70, scipen = 6, digits = 3)
library(knitr)
opts_chunk$set(fig.show='hold', size='footnotesize', comment="#", warning=FALSE, message=FALSE, dev='cairo_pdf', fig.height=2.5, fig.width=7.7)
```


### Вы сможете

- Подобрать модель множественной линейной регрессии
- Протестировать значимость модели и ее коэффициентов
- Интерпретировать коэффициенты множественной регрессии при разных предикторах
- Проверить условия применимости простой и множественной линейной регрессии при помощи анализа остатков

# Множественная линейная регрессия

## Пример: птицы Австралии

Зависит ли обилие птиц в лесах Австралии от характеристик леса? (Loyn, 1987, пример из кн. Quinn, Keough, 2002)

56 лесных участков в юго-восточной Виктории, Австралия

- `l10area` - Площадь леса, га
- `l10dist` - Расстояние до ближайшего леса, км (логарифм)
- `l10ldist` - Расстояние до ближайшего леса большего размера, км (логарифм)
- `yr.isol` - Продолжительности изоляции, лет
- `abund` - Обилие птиц

## Открываем данные

```{r}
# установите рабочую директорию
# birds <- read.delim(file = "data/loyn.csv") # из .csv
library(readxl)
birds <- read_excel("data/loyn.xls", sheet = 1)
str(birds)
```

## Задача

- Подберите модель множественной линейной регрессии, чтобы описать, как зависит обилие птиц от характеристик леса
- Проверьте значимость ее коэффициентов при помощи t-критерия

- `abund` - Обилие птиц
- `l10area` - Площадь леса, га
- `l10dist` - Расстояние до ближайшего леса, км (логарифм)
- `l10ldist` - Расстояние до ближайшего леса большего размера,
км (логарифм)
- `yr.isol` - Год изоляции лесного массива

## Решение

```{r}
bird_lm <- lm(abund ~ l10area + l10dist + l10ldist + yr.isol, data = birds)
summary(bird_lm)
```

## Задача

Запишите уравнение множественной линейной регрессии

В качестве подсказки:  

```{r}
coef(bird_lm)
bird_lm$call
```

```{r echo = FALSE}
lm_equation <- function(fit, strict = TRUE){
#   extracting call formula 
  frml <- as.character(fit$call)[2]
#   extract signs
    sign <- ifelse(grepl("-", coef(fit)[-1]), " - ", " + ")
  # extract coefficients
  coeffs <- format(abs(coef(fit)), digits = 2, trim = TRUE)
  if(strict == TRUE){
    i <- 1:(length(coeffs) - 1)
    vars <- c("Y", paste0(" X", i))
    
  } else {
# extract vector of variable names
  vars <- unlist(strsplit(frml, "[~+]"))
# combine everything
  }
  start <- ifelse(coef(fit)[1] > 0, paste(vars[1], coeffs[1], sep = " = "), paste(vars[1], coeffs[1], sep = " = - "))
  end <- paste(sign, coeffs[-1], vars[-1], sep = "", collapse = "")
  return(cat(start, end, sep = ""))
}
```

## Решение

Коэффициенты модели:

```{r}
coef(bird_lm)
```

Уравнение регрессии:  

```{r results='asis', echo=FALSE}
lm_equation(bird_lm, strict=FALSE)
```

Более формальная запись:  

```{r results='asis', echo=FALSE}
lm_equation(bird_lm)
```

## Интерпретация коэффициентов регрессии

```{r}
coef(bird_lm)
```

### Обычные коэффициенты

- Величина зависит от единиц измерения
- Коэффициент при $Х_p$ показывают, на сколько изменяется $Y$, когда предиктор $Х_p$ меняется на единицу, при условии, что остальные предикторы не меняют своих значений.


## Сравнение влияния разных факторов

```{r}
scaled_bird_lm <- lm(abund ~ scale(l10area) + scale(l10dist) + 
                       scale(l10ldist) + scale(yr.isol), data = birds)
coef(scaled_bird_lm)
```

### Бета-коэффициенты 
- Измерены в стандартных отклонениях
- Коэффициент при $Х_p$ показывают, на сколько изменяется $Y$, когда предиктор $Х_p$ меняется на одно стандартное отклонение, при условии, что остальные предикторы не меняют своих значений.
- Относительная оценка влияния фактора
- Можно сравнивать друг с другом


## Задача

Сравните влияние разных факторов

Определите по значениям beta-коэффициентов, какие факторы сильнее всего влияют на обилие птиц

\fontsize{10pt}{10pt}

```{r}
summary(scaled_bird_lm)
```

## Оценка качества подгонки модели

```{r}
summary(bird_lm)$adj.r.squared
```

### Скорректированный $R^2$
- Учитывает число переменных в модели

# Условия применимости линейной регрессии

## Условия применимости линейной регрессии 

Условия применимости линейной регрессии должны выполняться, чтобы тестировать гипотезы

1. Независимость
1. Линейность 
1. Нормальное распределение
1. Гомогенность дисперсий
1. Отсутствие колинеарности предикторов (для множественной регрессии)

## 1. Независимость


- Значения $y _i$ должны быть независимы друг от друга

- берегитесь псевдоповторностей и автокорреляций (например, временных)

- Контролируется на этапе планирования
- Проверяем на графике остатков


\includegraphics[width=\linewidth,height=\textheight,keepaspectratio]{images/assumption-12.png}
\vskip0pt plus 1filll
Остаточная изменчивость (Рис. из кн. Diez et al., 2010, стр. 332, рис. 7.8)

## 2. Линейность связи

- проверяем на графике рассеяния исходных данных
- проверяем на графике остатков

\includegraphics[width=\linewidth,height=\textheight,keepaspectratio]{images/assumption-12.png}
\vskip0pt plus 1filll
Остаточная изменчивость (Рис. из кн. Diez et al., 2010, стр. 332, рис. 7.8)

## Что бывает, если неглядя применять линейную регрессию

\columnsbegin
\column{0.48\textwidth}

\href{http://ru.wikipedia.org/wiki/Квартет_Энскомба}{Квартет Энскомба} - примеры данных, где регрессии одинаковы во всех случаях (Anscombe, 1973)

\[y _i = 3.0 + 0.5 x _i\]

\[r^2 = 0.68\]

\[H _0: \beta _1 = 0, t = 4.24, p = 0.002\]


\column{0.48\textwidth}

\centering
\includegraphics[width=\linewidth,height=\textheight,keepaspectratio]{images/anscombe.png}
\tiny{Квартет Энскомба (рис. из кн. Quinn, Keough, 2002, стр. 97, рис. 5.9}


\columnsend

## 3. Нормальное распределение остатков

\columnsbegin
\column{0.48\textwidth}
Нужно, т.к. в модели $Y _i = \beta _0 + \beta x _i + \epsilon _i$ зависимая переменная $Y \sim N(0,\sigma^2)$, а значит $\epsilon _i \sim N(0,\sigma^2)$

- Нужно для тестов параметров, а не для подбора методом наименьших квадратов
- Нарушение не страшно - тесты устойчивы к небольшим отклонениям от нормального распределения
- Проверяем распределение остатков на нормально-вероятностном графике


\column{0.48\textwidth}

\centering
\includegraphics[width=\linewidth,height=\textheight,keepaspectratio]{images/normality-assumption.png}
\tiny{Условие нормальности и гомогенность дисперсий (рис. 11.4 из кн. Watkins et al., 2008, стр. 743)}


\columnsend

## 4. Гомогенность дисперсий

\columnsbegin
\column{0.48\textwidth}
Нужно, т.к. в модели $Y _i = \beta _0 + \beta x _i + \epsilon _i$ зависимая переменная $Y \sim N(0,\sigma^2)$ и дисперсии $\sigma^2 _1 = \sigma^2 _2 = ... = \sigma^2 _i$ для каждого $Y _i$ \par
Но, поскольку $\epsilon _i \sim N(0,\sigma^2)$, можно проверить равенство дисперсий остатков $\epsilon _i$

- Нужно и важно для тестов параметров
- Проверяем на графике остатков по отношению к предсказанным значениям
- Есть формальные тесты (Cochran's C), но только если несколько значений y для каждого x


\column{0.48\textwidth}

\centering
\includegraphics[width=\linewidth,height=\textheight,keepaspectratio]{images/normality-assumption.png}
\tiny{Условие нормальности и гомогенность дисперсий (рис. 11.4 из кн. Watkins et al., 2008, стр. 743)}


\columnsend

## Диагностика регрессии по графикам остатков

\columnsbegin
\column{0.48\textwidth}

\centering
\includegraphics[width=\linewidth,height=\textheight,keepaspectratio]{images/assumption-violations-on-residual-plots.png}
\tiny{Диагностика регрессии по графикам остатков (рис. 8.5 d из кн. Logan, 2010, стр. 174)}


\column{0.48\textwidth}
\begin{enumerate}[(a)]
- все условия выполнены
- разброс остатков разный (wedge-shaped pattern)
- разброс остатков одинаковый, но нужны дополнительные предикторы
- к нелинейной зависимости применили линейную регрессию
\end{enumerate}

\columnsend

## Задача: Проанализируйте графики остатков

Скажите пожалуйста


- какой регрессии соответствует какой график остатков?
- все ли условия применимости регрессии здесь выполняются?
- назовите случаи, в которых можно и нельзя применить линейную регрессию?

\columnsbegin
\column{0.48\textwidth}

\centering
\includegraphics[width=\linewidth,height=\textheight,keepaspectratio]{images/assumption-quiz1.png}

\column{0.48\textwidth}
\includegraphics[width=\linewidth,height=\textheight,keepaspectratio]{images/assumption-quiz2.png}

\columnsend

\tiny{Из кн. Watkins et al. 2008, стр. 177, рис. 3.84-3.85}


## Решение

- A-I - нелинейная связь - нельзя; 
- B-II - все в порядке, можно; 
- C-III - все в порядке, можно; 
- D-IV - синусоидальный паттерн в остатках, нарушено условие независимости или зависимость нелинейная - нельзя.

\vskip0pt plus 1filll

\columnsbegin
\column{0.48\textwidth}
\centering
\includegraphics[width=\linewidth,height=\textheight,keepaspectratio]{images/assumption-quiz1.png}
\column{0.48\textwidth}

\includegraphics[width=\linewidth,height=\textheight,keepaspectratio]{images/assumption-quiz2.png}
\columnsend


\tiny Рис. из кн. Watkins et al. 2008, стр. 177, рис. 3.84-3.85


## Какие наблюдения влияют на ход регрессии больше других?

\columnsbegin
\column{0.48\textwidth}

Влиятельные наблюдения, выбросы, outliers

- большая абсолютная величина остатка
- близость к краям области определения (leverage - рычаг, сила; иногда называют hat)

На графике точки и линии регрессии построенные с их включением:

- 1 - не влияет на ход регрессии, т.к. лежит на прямой
- 2 - умеренно влияет (большой остаток, малая сила влияния)
- 3 - очень сильно влияет (большой остаток, большая сила влияния)


\column{0.48\textwidth}

\centering
\includegraphics[width=\linewidth,height=\textheight,keepaspectratio]{images/influential-observations.png}
\tiny{Влиятельные наблюдения (рис. 5.8 из кн. Quinn, Keough, 2002, стр. 96)}

\columnsend

## Как оценить влиятельность наблюдений?

\columnsbegin
\column{0.48\textwidth}

\blockbegin{Расстояние Кука (Cook's d, Cook, 1977)}

- Учитывает одновременно величину остатка и близость к краям области определения (leverage)
- Условное пороговое значение: выброс, если $d \ge 4/(N - k - 1)$, где $N$ - объем выборки, $k$ - число предикторов.

\blockend

\pause

- Дж. Фокс советует не обращать внимания на пороговые значения (Fox, 1991)

\column{0.48\textwidth}

\centering
\includegraphics[width=\linewidth,height=\textheight,keepaspectratio]{images/influential-observations.png}

\tiny{Влиятельные наблюдения (рис. 5.8 из кн. Quinn, Keough, 2002, стр. 96)}


\columnsend

## Что делать с влиятельными точками и с выбросами?

\columnsbegin
\column{0.48\textwidth}

- Проверить, не ошибка ли это. Если нет, не удалять - обсуждать!
- Проверить, что будет, если их исключить из модели


\column{0.48\textwidth}

\centering
\includegraphics[width=\linewidth,height=\textheight,keepaspectratio]{images/influential-observations.png}
\tiny{Влиятельные наблюдения (рис. 5.8 из кн. Quinn, Keough, 2002, стр. 96)}


\columnsend

## Колинеарность предикторов

\begin{block}{Колинеарность}
Когда предикторы коррелируют друг с другом, т.е. не являются взаимно независимыми
\end{block}

Последствия

- Модель неустойчива к изменению данных
- При добавлении или исключении наблюдений может меняться оценка и знак коэффициентов

Что делать с колинеарностью?

- Удалить из модели избыточные предикторы
- Получить вместо скоррелированных предикторов один новый комбинированный при помощи метода главных компонент

## Проверка на колинеарность

### Толерантность (tolerance)

$1-R^2$ регрессии данного предиктора от всех других

$T \le 0.25$ - колинеарность

### Показатель инфляции для дисперсии

(коэффициент распространения дисперсии, Variance inflation factor, VIF)

$VIF = 1/T$

$\sqrt{VIF} > 2$  - коллинеарность

# Проверка условий применимости линейной регрессии

## Как проверить условия применимости?

1. VIF --- колинеарность предикторов (для множественной регрессии)
2. График остатков от предсказанных значений --- величина остатков, влиятельность наблюдений, отсутствие паттернов, гомогенность дисперсий.
3. График квантилей остатков --- распределение остатков

## 1. Проверим, есть ли в этих данных колинеарность предикторов

```{r message = FALSE}
library(car)
vif(bird_lm) # variance inflation factors
sqrt(vif(bird_lm)) > 2 # есть ли проблемы?
1/vif(bird_lm) # tolerance
```

\pause
Все в порядке, предикторы независимы

## Для анализа остатков выделим нужные данные в новый датафрейм

```{r}
library(ggplot2) # там есть функция fortify()
bird_diag <- fortify(bird_lm)

head(bird_diag, 2)
```

- `.cooksd` - расстояние Кука  
- `.fitted` - предсказанные значения  
- `.resid` - остатки  
- `.stdresid` - стандартизованные остатки

## Задача

Постройте график зависимости стандартизованных остатков от предсказанных значений

Используйте данные из `bird_diag`

```{r, eval = FALSE}
ggplot()
aes()
geom_point()
```

### Стандартизованные остатки 

$$\frac {y _i - \hat y _i} {\sqrt{MS _e}}$$

- можно сравнивать между регрессиями
- можно сказать, какие остатки большие, какие нет
    - $\le 2 SD$ - обычные
    - $> 3 SD$ - редкие

## Решение

График зависимости стандартизованных остатков от предсказанных значений

```{r}
ggplot(data = bird_diag, aes(x = .fitted, y = .stdresid)) + geom_point()
```

## График станет информативнее, если кое-что добавить

```{r}
# Создаем логический вектор, где TRUE, 
# если стандартизованный остаток больше 2
f_outlier <- abs(bird_diag$.stdresid) > 2
# Создаем будущие ярлыки
labs <- ifelse(test = f_outlier,
               yes = row(bird_diag), # Если test == TRUE
               no = "") # Если test == FALSE

gg_resid <- ggplot(data = bird_diag, 
                   aes(x = .fitted, y = .stdresid)) + 
  geom_point(aes(size = .cooksd)) + # расстояние Кука
  geom_hline(yintercept = 0) +   # горизонтальная линия y = 0
  geom_text(aes(label = labs), hjust = 2, colour = "blue", 
            size = 2) # номера наблюдений с остатками больше 2SD
```

## Интерпретируем график стандартизованных остатков от предсказанных значений

Какие выводы можно сделать по графику остатков?

```{r echo=FALSE}
gg_resid
```

\pause

- Большая часть стандартизованных остатков в пределах двух стандартных отклонений. Есть отдельные влиятельные наблюдения, которые нужно проверить
- Разброс остатков не совсем одинаков. Похоже на гетерогенность дисперсий
- Тренда среди остатков нет


## 3. Нормальновероятностный график стандартизованных остатков

Используется, чтобы оценить форму распределения.
Если точки лежат на одной прямой - нормальное распределение.

```{r qqplot, warning = FALSE, message=FALSE}
mean_val <- mean(bird_diag$.stdresid)  
sd_val <- sd(bird_diag$.stdresid)
ggplot(bird_diag, aes(sample = .stdresid)) + geom_point(stat = "qq") +
geom_abline(intercept = mean_val, slope = sd_val) + # точки должны быть здесь
  labs(x = "Квантили стандартного нормального распределения", y = "Квантили набора данных")
```

## Интерпретируем нормальновероятностный график 

Какие выводы можно сделать по нормальновероятностному графику?

```{r qqplot, warning = FALSE, message=FALSE, echo=FALSE}
```
\pause

- Отклонений от нормального распределения нет




## Take home messages

- Для сравнения влияния разных предикторов можно использовать бета-коэффициенты
- Условия применимости линейной регрессии должны выполняться, чтобы тестировать гипотезы
    1. Независимость
    1. Линейность 
    1. Нормальное распределение
    1. Гомогенность дисперсий
    1. Отсутствие колинеарности предикторов (для множественной регрессии)

## Дополнительные ресурсы

Учебники

- Quinn, Keough, 2002, pp. 92-98, 111-130
- [Open Intro to Statistics](https://docs.google.com/viewer?docex=1&url=http://www.openintro.org/stat/down/OpenIntroStatSecond.pdf): [Chapter 8. Multiple and logistic regression](https://docs.google.com/viewer?docex=1&url=http://www.openintro.org/stat/down/oiStat2_08.pdf), pp. 354-367.
- Logan, 2010, pp. 170-173, 208-211
- Sokal, Rohlf, 1995, pp. 451-491, 609-653
- Zar, 2010, pp. 328-355, 419-439

Упражнения для тренировки

- OpenIntro Labs, Lab 7: Introduction to linear regression (Осторожно, они используют базовую графику а не `ggplot`)
    - [Обычный вариант](http://www.openintro.org/download.php?file=os2_lab_07A&referrer=/stat/labs.php), после упражнения 4
    - [Интерактивный вариант на Data Camp](https://www.datacamp.com/courses/data-analysis-and-statistical-inference_mine-cetinkaya-rundel-by-datacamp/lab-6-introduction-to-linear-regression?ex=1), после вопроса 4
- OpenIntro Labs, Lab 8: Multiple linear regression
    - [Обычный вариант](http://www.openintro.org/download.php?file=os2_lab_08A&referrer=/stat/labs.php), до упражнения 11
    - [Интерактивный вариант на Data Camp](https://www.datacamp.com/courses/data-analysis-and-statistical-inference_mine-cetinkaya-rundel-by-datacamp/lab-7-multiple-linear-regression-9?ex=1), до вопроса 8
    
