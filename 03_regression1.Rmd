---
title: "Регрессионный анализ, часть 1"
subtitle: "Математические методы в зоологии с использованием R"
author: "Марина Варфоломеева"
output:
  beamer_presentation:
    colortheme: seagull
    highlight: tango
    fonttheme: structurebold
    includes:
      in_header: ./includes/header.tex
    pandoc_args:
    - --latex-engine=xelatex
    - -V fontsize=10pt
    - -V lang=russian
    slide_level: 2
    theme: CambridgeUS
    toc: yes
---

```{r setup, include = FALSE, cache = FALSE, purl = FALSE}
options(width = 70, scipen = 6, digits = 3)
library(knitr)
opts_chunk$set(fig.show='hold', size='footnotesize', comment="#", warning=FALSE, message=FALSE, dev='cairo_pdf', fig.height=2.5, fig.width=7.7)
```

### Вы сможете

- посчитать и протестировать различные коэффициенты корреляции между переменными
- подобрать модель линейной регрессии и записать ее в виде уравнения
- интерпретировать коэффициенты простой линейной регрессии
- протестировать значимость модели и ее коэффициентов при помощи t- или F-теста
- оценить долю изменчивости, которую объясняет модель, при помощи $R^2$

# Описание зависимости между переменными

## Пример: потеря влаги личинками мучных хрущаков

\columnsbegin

\column{0.48\textwidth}

Как зависит потеря влаги личинками [малого мучного хрущака](http://ru.wikipedia.org/wiki/Хрущак_малый_мучной) _Tribolium confusum_ от влажности воздуха? 

- 9 экспериментов, продолжительность 6 дней
- разная относительная влажность воздуха, %
- измерена потеря влаги, мг

\column{0.48\textwidth}

\centering

\includegraphics[width=0.3\linewidth]{images/Tribolium_confusum.png}

\vfill
\tiny Малый мучной хрущак \textit{Tribolium confusum}, photo by Sarefo, CC BY-SA

\columnsend

\vfill
\tiny Nelson, 1964; данные из Sokal, Rohlf, 1997, табл. 14.1 по Logan, 2010. глава 8, пример 8c; Данные в файлах nelson.xlsx и nelson.csv

## Скачиваем данные с сайта

Не забудьте войти в вашу директорию для матметодов при помощи `setwd()`

```{r eval=FALSE}
library(downloader)

# в рабочем каталоге создаем суб-директорию для данных
if(!dir.exists("data")) dir.create("data")

# скачиваем файл в xlsx, либо в текстовом формате
if (!file.exists("data/nelson.xlsx")) {
  download(
    url = "https://varmara.github.io/mathmethr/data/nelson.xlsx",
    destfile = "data/nelson.xlsx")
}

if (!file.exists("data/nelson.csv")) {
  download(
    url = "https://varmara.github.io/mathmethr/data/nelson.xls",
    destfile = "data/nelson.csv")
}
```


## Читаем данные из файла одним из способов

### Чтение из xlsx
```{r}
library(readxl)
nelson <- read_excel(path = "data/nelson.xlsx", sheet = 1)
```

### Чтение из csv

```{r}
nelson <- read.table("data/nelson.csv", header = TRUE, sep = "\t")
```

## Все ли правильно открылось?

```{r}
str(nelson) # Структура данных
head(nelson)     # Первые несколько строк файла
```

## Знакомимся с данными

Есть ли пропущенные значения?

```{r}
sapply(nelson, function(x)sum(is.na(x)))
```

Каков объем выборки?

```{r}
nrow(nelson)
```

## Как зависит потеря веса от влажности?

```{r, nelson-plot}
library(ggplot2)
theme_set(theme_bw())
gg_nelson <- ggplot(data=nelson, aes(x = humidity, y = weightloss)) + 
  geom_point() + 
  labs(x = "Относительная влажность, %",
       y = "Потеря веса, мг")
gg_nelson
```

## Коэффициент корреляции --- способ оценки силы связи между двумя переменными

### Коэффициент корреляции Пирсона

- Оценивает только линейную составляющую связи
- Параметрические тесты (t-критерий) значимости применимы если переменные распределены нормально
  
  
### Ранговые коэффициенты корреляции (кор. Кендалла и кор. Спирмена)

- Не зависят от формы распределения переменных
- Тест на значимость непараметрический

## Интерпретация коэффициента корреляции

\columnsbegin
\column{0.33\textwidth}
$$-1 < \rho < 1$$
\column{0.33\textwidth}
$|\rho| = 1$ --- сильная связь
\column{0.33\textwidth}
$\rho = 0$ --- нет связи
\columnsend

> - В тестах для проверки значимости тестируется гипотеза $H_0: \rho = 0$


```{r echo=FALSE, fig.height=4, out.height='4in', purl=FALSE}
#Title: An example of the correlation of x and y for various distributions of (x,y) pairs
#Tags: Mathematics; Statistics; Correlation
#Author: Denis Boigelot
#Packets needed : mvtnorm (rmvnorm), RSVGTipsDevice (devSVGTips)
#How to use: output()
#
#This is an translated version in R of an Matematica 6 code by Imagecreator.

library(mvtnorm)
# library(RSVGTipsDevice)

MyPlot <- function(xy, xlim = c(-4, 4), ylim = c(-4, 4), eps = 1e-15) {
   title = round(cor(xy[,1], xy[,2]), 1)
   if (sd(xy[,2]) < eps) title = "" # corr. coeff. is undefined
   plot(xy, main = title, xlab = "", ylab = "",
        col = "darkblue", pch = 16, cex = 0.2,
        xaxt = "n", yaxt = "n", bty = "n",
        xlim = xlim, ylim = ylim)
}

MvNormal <- function(n = 1000, cor = 0.8) {
   for (i in cor) {
      sd = matrix(c(1, i, i, 1), ncol = 2)
      x = rmvnorm(n, c(0, 0), sd)
      MyPlot(x)
   }
}

rotation <- function(t, X) return(X %*% matrix(c(cos(t), sin(t), -sin(t), cos(t)), ncol = 2))

RotNormal <- function(n = 1000, t = pi/2) {
   sd = matrix(c(1, 1, 1, 1), ncol = 2)
   x = rmvnorm(n, c(0, 0), sd)
   for (i in t)
      MyPlot(rotation(i, x))
}

Others <- function(n = 1000) {
   x = runif(n, -1, 1)
   y = 4 * (x^2 - 1/2)^2 + runif(n, -1, 1)/3
   MyPlot(cbind(x,y), xlim = c(-1, 1), ylim = c(-1/3, 1+1/3))

   y = runif(n, -1, 1)
   xy = rotation(-pi/8, cbind(x,y))
   lim = sqrt(2+sqrt(2)) / sqrt(2)
   MyPlot(xy, xlim = c(-lim, lim), ylim = c(-lim, lim))

   xy = rotation(-pi/8, xy)
   MyPlot(xy, xlim = c(-sqrt(2), sqrt(2)), ylim = c(-sqrt(2), sqrt(2)))
   
   y = 2*x^2 + runif(n, -1, 1)
   MyPlot(cbind(x,y), xlim = c(-1, 1), ylim = c(-1, 3))

   y = (x^2 + runif(n, 0, 1/2)) * sample(seq(-1, 1, 2), n, replace = TRUE)
   MyPlot(cbind(x,y), xlim = c(-1.5, 1.5), ylim = c(-1.5, 1.5))

   y = cos(x*pi) + rnorm(n, 0, 1/8)
   x = sin(x*pi) + rnorm(n, 0, 1/8)
   MyPlot(cbind(x,y), xlim = c(-1.5, 1.5), ylim = c(-1.5, 1.5))

   xy1 = rmvnorm(n/4, c( 3,  3))
   xy2 = rmvnorm(n/4, c(-3,  3))
   xy3 = rmvnorm(n/4, c(-3, -3))
   xy4 = rmvnorm(n/4, c( 3, -3))
   MyPlot(rbind(xy1, xy2, xy3, xy4), xlim = c(-3-4, 3+4), ylim = c(-3-4, 3+4))
}

output <- function() {
   # devSVGTips(width = 7, height = 3.2) # remove first and last line for no svg exporting
   par(mfrow = c(3, 7), oma = c(0,0,0,0), mar=c(2,2,2,0))
   MvNormal(800, c(1.0, 0.8, 0.4, 0.0, -0.4, -0.8, -1.0));
   RotNormal(200, c(0, pi/12, pi/6, pi/4, pi/2-pi/6, pi/2-pi/12, pi/2));
   Others(800)
   # dev.off() # remove first and last line for no svg exporting
}
output()
```

\vfill
\tiny
\href{https://commons.wikimedia.org/wiki/File\%3ACorrelation\\_examples2.svg}{By DenisBoigelot, original uploader was Imagecreator} [CC0], via Wikimedia Commons

## Можно расчитать значение коэффициента корреляции между потерей веса и влажностью

\fontsize{10pt}{10pt}

```{r}
p_cor <- cor.test(nelson$humidity, nelson$weightloss, 
         alternative = "two.sided", method = "pearson")
p_cor
```

\pause

Можно описать результаты несколькими способами:

- Величина потери веса мучных хрущаков отрицательно коррелирует с относительной влажностью воздуха ($r = `r round(p_cor$estimate, 2)`$, $p `r format.pval(p_cor$p.value, eps = 0.01)`$)
- Мучные хрущаки теряют вес при уменьшении относительной влажности воздуха ($r = `r round(p_cor$estimate, 2)`$, $p `r format.pval(p_cor$p.value, eps = 0.01)`$)


## Коэффициент корреляции не позволяет предсказать значение одной переменной, зная знаячение другой

Нам бы хотелось описать функциональную зависимость

$$weightloss _i = b _0 + b _1 humidity _i$$

```{r, nelson-plot, echo=FALSE, purl=FALSE}
```

# Линейная регрессия

## Линейная регрессия

- простая

$$Y _i = \beta _0 + \beta _1 x _i + \varepsilon _i$$

- множественная

$$Y _i = \beta _0 + \beta _1 x _{1 i} + \beta _2 x _{2 i} + ... + \varepsilon _i$$

## Как провести линию регрессии?

Линейная модель:

$$Y _i = \beta _0 + \beta _1 x _i + \varepsilon _i$$

Оценка модели:

$$\hat y _i = b _0 + b _1 x _i$$

Нужно оценить параметры линейной модели:

- $\beta _0$
- $\beta _1$

Методы оценки параметров:

- Метод наименьших квадратов (Ordinary Least Squares)
- Методы максимального правдоподобия (Maximum Likelihood, REstricted Maximum Likelihood)


## Метод наименьших квадратов

$$\hat y _i = b _0 + b _1 x _i$$

\columnsbegin

\column{0.4\textwidth}

Оценки параметров линейной регрессии подбирают так, чтобы минимизировать остатки $\sum{(y _i - \hat y _i)^2}$


\column{0.6\textwidth}

\includegraphics[width=0.9\linewidth]{images/OLS-regression-line.png}

Линия регрессии по методу наименьших квадратов

\vfill
\tiny из кн. Quinn, Keough, 2002, стр. 85, рис. 5.6 a

\columnsend

## Оценки параметров линейной регрессии

\begin{tabular}{l l l}
 \hline\noalign{\smallskip}
Параметры & Оценки параметров & Стандартные ошибки оценок \\
 \hline\noalign{\smallskip}
$\beta _1$    & $b _1 = \frac {\sum _{i=1}^{n} {[(x _i - \bar x)(y _i - \bar y)]}}{\sum _{i=1}^{n} {(x _i - \bar x)^2}}$      & $SE _{b _1} = \sqrt{\frac{MS _e}{\sum _{i=1}^{n} {(x _i - \bar x)^2}}}$ \\
$\beta _0$    & $b _0 = \bar y - b _1 \bar x$  & $SE _{b _0} = \sqrt{MS _e [\frac{1}{n} + \frac{\bar x}{\sum _{i=1}^{n} {(x _i - \bar x)^2}}]}$ \\
 \hline\noalign{\smallskip}
\end{tabular}

\tiny Таблица из кн. Quinn, Keough, 2002, стр. 86, табл. 5.2
\normalsize

\vfill

Стандартные ошибки коэффициентов
  - используются для построения доверительных интервалов
  - нужны для статистических тестов

## Интерпретация коэффициентов регрессии

\includegraphics[width=\linewidth]{images/interpretation-of-regression-coefficients.png}

\tiny Рисунок из кн. Logan, 2010, стр. 170, рис. 8.2
\normalsize

\vfill

\pause

- $b_0$ --- Отрезок (Intercept), отсекаемый регрессионной прямой на оси $y$. Значение зависимой переменной $y$, если предиктор $x = 0$.
- $b_1$ --- Коэффициент угла наклона регрессионной прямой. Показывает на сколько единиц изменяется отклик ($y$), при увеличении значения предиктора ($x$) на единицу.


## Для сравнения разных моделей - стандартизованные коэффициенты

- Не зависят от масштаба измерений x и y
- Можно вычислить, зная обычные коэффициенты и их стандартные отклонения $b^\ast _1 = {b _1  \frac {\sigma _x} {\sigma _y}}$
- Можно вычислить, посчитав регрессию по стандартизованным данным

## Добавим линию регрессии на график

```{r, nelson-conf}
gg_nelson + geom_smooth(method = "lm")
```

Что это за серая область вокруг линии регрессии?

\pause

### Доверительная зона регрессии

- 95\% доверительная зона регрессии
- В ней с 95\% вероятностью лежит регрессионная прямая  
- Возникает из-за неопределенности оценок коэффициентов регрессии

## Как в R задать формулу линейной регрессии

`lm(формула, данные)` - функция для подбора регрессионных моделей

Формат формулы: `зависимая_переменная ~ модель`

- $\hat y _i = b _0 + b _1 x _i$ (простая линейная регрессия с $b _0$ (intercept))
    - Y ~ X
    - Y ~ 1 + X 
    - Y ~ X + 1

- $\hat y _i = b _1 x _i$ (простая линейная регрессия без $b _0$)
    - Y ~ X - 1
    - Y ~ -1 + X

- $\hat y _i = b _0$ (уменьшенная модель, линейная регрессия Y от $b _0$)
    - Y ~ 1
    - Y ~ 1 - X

## Задача


Запишите в нотации R эти модели линейных регрессий

- $\hat y _i = b _0 + b _1 x _{1 i} + b _2 x _{2 i} + b _3 x _{3 i}$

(множественная линейная регрессия с $b _0$)

- $\hat y _i = b _0 + b _1 x _{1 i} + b _3 x _{3 i}$

(уменьшенная модель множественной линейной регрессии, без $x _2$)

## Решение

- $\hat y _i = b _0 + b _1 x _{1 i} + b _2 x _{2 i} + b _3 x _{3 i}$

(множественная линейная регрессия с $b _0$)

    Y ~ X1 + X2 + X3
    
    Y ~ 1 + X1 + X2 + X3

- $\hat y _i = b _0 + b _1 x _{1 i} + b _3 x _{3 i}$

(уменьшенная модель множественной линейной регрессии, без $x _2$)

    Y ~ X1 + X3
    
    Y ~ 1 + X1 + X3

## Подбираем параметры линейной модели

\fontsize{10pt}{10pt}

```{r, nelson-reg}
nelson_lm <- lm(weightloss ~ humidity, nelson)
summary(nelson_lm)
```

\pause

Коэффициенты линейной регрессии:

- $b _0 =  `r round(coef(nelson_lm)[1], 2)`$
- $b _1 =  `r round(coef(nelson_lm)[2], 2)`$

# Неопределенность оценок коэффициентов

## Неопределенность оценок коэффициентов

### Доверительный интервал коэффициента
  - зона, в которой с $(1 - \alpha) \cdot 100\%$ вероятностью содержится среднее значение коэффициента
  - $b _1 \pm t _{\alpha, df = n - 2} \cdot SE _{b _1}$
  - $\alpha = 0.05$ => $(1 - 0.05) \cdot 100\% = 95\%$ интервал

### Доверительная зона регрессии
  - зона, в которой с $(1 - \alpha) \cdot 100\%$ вероятностью лежит регрессионная прямая

```{r, nelson-conf, echo=FALSE, eval=FALSE, purl=FALSE}
```

## Находим доверительные интервалы коэффициентов

```{r}
# оценки коэффициентов отдельно
coef(nelson_lm)

# доверительные интервалы коэффициентов
confint(nelson_lm)
```

## Предсказываем Y при заданном X 

Какова средняя потеря веса при заданной влажности?

\fontsize{10pt}{10pt}

```{r fig.height = 7}
newdata <- data.frame(humidity = c(50, 100)) # значения, для которых предсказываем
(pr1 <- predict(nelson_lm, newdata, interval = "confidence", se = TRUE))
```

\pause

- При 50 и 100\% относительной влажности ожидаемая средняя потеря веса жуков будет `r round(pr1$fit[1,1], 1)` $\pm$ `r round(pr1$fit[1,1] - pr1$fit[1,2], 1)` и `r round(pr1$fit[2,1], 1)` $\pm$ `r round(pr1$fit[2,1] - pr1$fit[2,2], 1)`, соответственно.

## Строим доверительную зону регрессии

```{r eval=FALSE}
gg_nelson + geom_smooth(method = "lm") + 
  labs (title = "95% доверительная зона регрессии")

gg_nelson + geom_smooth(method = "lm", level = 0.99) + 
  labs (title = "99% доверительная зона регрессии")
```

```{r echo = FALSE, purl=FALSE}
library(gridExtra)
grid.arrange(
gg_nelson + geom_smooth(method = "lm") + 
  labs (title = "95% доверительная зона регрессии"),
gg_nelson + geom_smooth(method = "lm", level = 0.99) + 
  labs (title = "99% доверительная зона регрессии"),
ncol = 2)
```


## Неопределенность оценок предсказанных значений

### Доверительный интервал к предсказанному значению
  - зона в которую попадают $(1 - \alpha) \cdot 100\%$ значений $\hat y _i$ при данном $x _i$
  - $\hat y _i \pm t _{\alpha, n - 2} \cdot SE _{\hat y _i}$
  - $SE _{\hat y} = \sqrt{MS _{e} [1 + \frac{1}{n} + \frac{(x _{prediction} - \bar x)^2} {\sum _{i=1}^{n} {(x _{i} - \bar x)^2}}]}$

### Доверительная область значений регрессии
  - зона, в которую попадает $(1 - \alpha) \cdot 100\%$ всех предсказанных значений

```{r, nelson-pr-all, echo=FALSE, results='hide', purl=FALSE}
(pr_all <- predict(nelson_lm, interval = "prediction"))
nelson_with_pred <- data.frame(nelson, pr_all)
```

```{r, nelson-pred, echo=FALSE, fig.height=1.75, purl=FALSE}
gg_nelson + geom_smooth(method = "lm", se = FALSE) +
  geom_ribbon(data = nelson_with_pred, 
              aes(y = fit, ymin = lwr, ymax = upr), 
              fill = 'green', alpha = 0.2)
```


## Предсказываем изменение Y для 95\% наблюдений при заданном X

В каких пределах находится потеря веса у 95\% жуков при заданной влажности?

\fontsize{10pt}{10pt}

```{r}
newdata <- data.frame(humidity = c(50, 100)) # новые данные для предсказания значений
(pr2 <- predict(nelson_lm, newdata, interval = "prediction", se = TRUE))
```

\pause

- У 95\% жуков при 50 и 100\% относительной влажности будет потеря веса будет в пределах `r round(pr2$fit[1,1], 1)` $\pm$ `r round(pr2$fit[1,1] - pr2$fit[1,2], 1)` и `r round(pr2$fit[2,1], 1)` $\pm$ `r round(pr2$fit[2,1] - pr2$fit[2,2], 1)`, соответственно.

## Данные для доверительной области значений

Предсказанные значения для исходных данных объединим с исходными данными в новом датафрейме - для графиков

```{r, nelson-pr-all}
```


## Строим доверительную область значений и доверительный интервал одновременно

```{r, nelson-plot-all}
gg_nelson + 
  geom_smooth(method = "lm", 
              aes(fill = "Доверительный \nинтервал"), 
              alpha = 0.4) +
  geom_ribbon(data = nelson_with_pred, 
              aes(y = fit, ymin = lwr, ymax = upr, 
                  fill = "Доверительная \nобласть значений"), 
              alpha = 0.2) +
  scale_fill_manual('Интервалы', values = c('green', 'blue'))
```

## Осторожно!

### Вне интервала значений $X$ ничего предсказать нельзя!
```{r echo=FALSE, fig.height=3, out.height='3in', purl=FALSE}
gg_nelson + 
  geom_smooth(method = "lm", 
              aes(fill = "Доверительный \nинтервал"), 
              alpha = 0.4) +
  geom_ribbon(data = nelson_with_pred, 
              aes(y = fit, ymin = lwr, ymax = upr, 
                  fill = "Доверительная \nобласть значений"), 
              alpha = 0.2) +
  scale_fill_manual('Интервалы', values = c('green', 'blue')) + xlim(-10, 110)
```

# Тестирование значимости модели и ее коэффициентов

## Тестируем коэффициенты t-критерием

### t-критерий

$$t = \frac{b _1 - \theta}{SE _{b _1}}$$

$H _0 : b _1 = \theta$, для $\theta = 0$

Число степеней свободы $df = n - 2$

## Тестируем значимость коэффициентов с помощью t-критерия

\fontsize{10pt}{10pt}

```{r}
summary(nelson_lm)
```

\pause

Результаты можно описать в тексте так:

- Увеличение относительной влажности привело к достоверному замедлению потери веса жуками ($b _1 = -0.053$, $t = - 16.35$, $p < 0.01$)

## Проверка при помощи F-критерия

### F-критерий

$$F = {MS _{regression} \over MS _{error}}$$

$H _0: \beta _1 = 0$ 

Число степеней свободы $df _{regression}$, $df _{error}$


## Общая изменчивость

Общая изменчивость - $SS _{total}$, отклонения от общего среднего значения

\centering
\includegraphics[width=.5\textwidth]{images/total-variation.png}

\raggedright
\vfill
\tiny Рис. из кн. Logan, 2010, стр. 172, рис. 8.3


## Общая изменчивость

$SS _{total} = SS _{regression} + SS _{error}$

\columnsbegin
\column{0.48\linewidth}
\centering
\includegraphics[width=.9\textwidth]{images/explained-variation.png}

Объясненная изменчивость

\column{0.48\linewidth}
\centering
\includegraphics[width=.9\textwidth]{images/residual-variation.png}

Остаточная изменчивость

\columnsend

\vfill
\tiny Рис. из кн. Logan, 2010, стр. 172, рис. 8.3

## Если зависимости нет, $b _1 = 0$

Тогда $\hat y _i = \bar y _i$ и $MS _{regression} \approx MS _{error}$

\columnsbegin
\column{0.48\linewidth}
\centering
\includegraphics[width=.9\textwidth]{images/explained-variation.png}

Объясненная изменчивость

\column{0.48\linewidth}
\centering
\includegraphics[width=.9\textwidth]{images/residual-variation.png}

Остаточная изменчивость

\columnsend

\vfill
\tiny Рис. из кн. Logan, 2010, стр. 172, рис. 8.3

## Что оценивают средние квадраты отклонений?

\resizebox{1\textwidth}{!}{
\begin{tabular}{ L{2.3cm} C{2cm} C{1.5cm} C{2.5cm} C{4cm}}
\hline\noalign{\smallskip}
Источник \linebreak[4] изменчивости & Суммы квадратов отклонений SS &  Число степеней свободы df & Средний квадрат отклонений MS & Ожидаемый средний квадрат \\
\hline\noalign{\smallskip}
Регрессия & $\sum{(\bar y - \hat y _i)^2}$ & $1$ & $\frac{\sum _{i=1}^{n}{(\bar y - \hat y _i)^2}}{1}$ & $\sigma _{\varepsilon} ^2 + {\beta _1} ^2 \sum _{i=1}^{n} {(x _i - \bar x)^2}$ \\
Остаточная & $\sum{(y _i - \hat y _i)^2}$ & $n - 2$ & $\frac{\sum _{i=1}^{n}{(y _i - \hat y _i)^2}}{n - 2}$ & $\sigma _{\varepsilon} ^2$ \\
Общая & $\sum {(\bar y - y _i)^2}$ & $n - 1$ &  & \\
\hline\noalign{\smallskip}
\end{tabular}
}


Если $b _1 = 0$, тогда $\hat y _i = \bar y _i$ и $MS _{regression} \approx MS _{error}$

Тестируем:

$$F = {MS _{regression} \over MS _{error}}$$

## F-критерий и распределение F-статистики

\columnsbegin
\column{0.38\linewidth}
F - соотношение объясненной и не объясненной изменчивости
$$F = \frac {MS _{regression}} {MS _{error}}$$
Зависит от

- $\alpha$
- $df _{regression}$
- $df _{error}$


\column{0.58\linewidth}
\centering
\includegraphics[width=.9\textwidth]{images/f-distribution.png}

Распределение F-статистики при справедливой $H _0$

\vfill
\tiny Рис. из кн. Logan, 2010, стр. 172, рис. 8.3

\columnsend


## Таблица результатов дисперсионного анализа

\resizebox{1\textwidth}{!}{
\begin{tabular}{L{2.2cm} c c c c}
\hline\noalign{\smallskip}
Источник \linebreak[2] изменчивости  & SS & df & MS & F  \\
\hline\noalign{\smallskip}
Регрессия & $SS _r = \sum{(\bar y - \hat y _i)^2}$ & $df _r = 1$ & $MS _r = \frac{SS _r}{df _r}$ & $F _{df _r, df _e} = \frac{MS _r}{MS _e}$ \\
Остаточная & $SS _e = \sum{(y _i - \hat y _i)^2}$ & $df _e = n - 2$ & $MS _e = \frac{SS _e}{df _e}$ \\
Общая & $SS _t = \sum {(\bar y - y _i)^2}$ & $df _t = n - 1$ & & \\
\hline\noalign{\smallskip}
\end{tabular}
}

\large Минимальное упоминание результатов в тексте должно содержать $F _{df _r, df _e}$ и $p$.

## Проверяем значимость модели при помощи F-критерия

```{r}
nelson_aov <- aov(nelson_lm)
summary(nelson_aov)
```

Результаты дисперсионного анализа можно описать в тексте:

- Количество влаги, потерянной жуками в период эксперимента, достоверно зависело от уровня относительной влажности \linebreak[2] ($F _{1, 7} = 267$, $p < 0.01$).

## Результаты дисперсионного анализа можно представить в виде таблицы

- Количество влаги, потерянной жуками в период эксперимента, достоверно зависело от уровня относительной влажности \linebreak[2] (Табл. \autoref{tab:reg-anova}).

```{r echo=FALSE, results='asis', purl=FALSE}
fix_aov_smr <- function(aov_obj, lang = "en", rown = NULL, coln = NULL){
  pvals <- aov_obj[[1]][, ncol(aov_obj[[1]])]
  pvals <- format.pval(pvals, eps = 0.01)
  pvals[pvals == "NA"] <- NA
aov_obj[[1]][, ncol(aov_obj[[1]])] <- pvals
if(!is.null(rown)){
  rownames(aov_obj[[1]]) <- rown
}
if(!is.null(coln)){
  colnames(aov_obj[[1]]) <- coln
}
return(aov_obj)
}

library(xtable)
rown <- c("Влажность", "Остаточная")
coln <- c("Ст.св.", "Сум.кв.", "Сред.кв.", "F критерий", "P")
capt <- "Результаты дисперсионного анализа зависимости потери веса мучных хрущаков от относительной влажности воздуха. Ст.св. --- число степеней свободы, Сум.кв. --- суммы квадратов, Сред. кв. --- Средние квадраты, F --- значение F-критерия, P --- доверительная вероятность."
lab <- "tab:reg-anova"
smr <- fix_aov_smr(summary(nelson_aov), rown = rown, coln = coln)
xtb <- xtable(smr, caption = capt, label = lab)
print.xtable(xtb, comment = F, caption.placement = "top")
```


# Оценка качества подгонки модели

## Коэффициент детерминации

### Коэффициент детерминации $R^2$

доля общей изменчивости, объясненная линейной связью x и y

$$R^2 =\frac{SS_{model}}{SS_{total}} = 1 - \frac{SS_{error}}{SS_{total}}$$

$$0 \le R^2 \le 1$$

Иначе рассчитывается как квадрат коэффициента корреляции $R^2 = r^2$

__Не используйте обычный $R^2$ для множественной регрессии!__


## Коэффициент детерминации можно найти в сводке модели

\fontsize{10pt}{10pt}
```{r}
summary(nelson_lm)
```

## Сравнение качества подгонки моделей

### $R^2_{adj}$ --- cкорректированный $R^2$

$$R^2_{adj} = 1 - \frac{SS_{error} / df_{error}}{SS_{total} / df_{total}}$$

где $df_{error} = n - p - 1$, $df_{total} = n - 1$

$R^2_{adj}$ учитывает число переменных в модели, вводится штраф за каждый новый параметр.

Используйте $R^2 _{adj}$ для сравнения моделей с разным числом параметров.


## Take home messages

- Модель простой линейной регрессии $y _i = \beta _0 + \beta _1 x _i + \varepsilon _i$
- В оценке коэффициентов регрессии и предсказанных значений существует неопределенность. Доверительные интервалы можно расчитать, зная стандартные ошибки.
- Значимость всей регрессии и ее параметров можно проверить при помощи t- или F-теста. $H _0: \beta _1 = 0$
- Качество подгонки модели можно оценить при помощи коэффициента детерминации $R^2$

## Дополнительные ресурсы

- Учебники
    - Гланц, 1999, стр. 221-244
    - [Open Intro to Statistics](https://docs.google.com/viewer?docex=1&url=http://www.openintro.org/stat/down/OpenIntroStatSecond.pdf): [Chapter 7. Introduction to linear regression](https://docs.google.com/viewer?docex=1&url=http://www.openintro.org/stat/down/oiStat2_07.pdf), pp. 315-353.
    - Quinn, Keough, 2002, pp. 78-110
    - Logan, 2010, pp. 170-207
    - Sokal, Rohlf, 1995, pp. 451-491
    - Zar, 1999, pp. 328-355

- Упражнения для тренировки
    - OpenIntro Labs, Lab 7: Introduction to linear regression (Осторожно, они используют базовую графику а не `ggplot`)
        - [Обычный вариант](http://www.openintro.org/stat/labs.php), упражнения 1---4
        - [Интерактивный вариант на Data Camp](https://www.datacamp.com/courses/data-analysis-and-statistical-inference_mine-cetinkaya-rundel-by-datacamp/lab-6-introduction-to-linear-regression?ex=1), до вопроса 4

